{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d64ec4",
   "metadata": {
    "_cell_guid": "64e3c1f8-79ff-433e-afae-b5c3de1760c2",
    "_uuid": "9a978044-b2fa-44b5-830b-f8db2fcf5007",
    "papermill": {
     "duration": 0.007822,
     "end_time": "2024-09-30T17:35:01.349916",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.342094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472bd402",
   "metadata": {
    "papermill": {
     "duration": 0.006748,
     "end_time": "2024-09-30T17:35:01.364536",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.357788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **Install**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6dfaa76",
   "metadata": {
    "_cell_guid": "a4780a91-70e4-4f8f-a190-3d2da268ba4f",
    "_uuid": "0d47750d-c119-4dd3-bb6a-5495de6c397b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.379726Z",
     "iopub.status.busy": "2024-09-30T17:35:01.379246Z",
     "iopub.status.idle": "2024-09-30T17:35:01.396643Z",
     "shell.execute_reply": "2024-09-30T17:35:01.395608Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028079,
     "end_time": "2024-09-30T17:35:01.399408",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.371329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install -r \"requirements.txt\" # install dependencies into file.txt\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install -r \"requirements.txt\" # install dependencies into file.txt\n",
    "\"\"\"\n",
    "\n",
    "#!pip uninstall \"package_individual\" -y # uninstall line with acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80dbf3f",
   "metadata": {
    "papermill": {
     "duration": 0.006418,
     "end_time": "2024-09-30T17:35:01.412801",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.406383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62cf4352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.428965Z",
     "iopub.status.busy": "2024-09-30T17:35:01.427850Z",
     "iopub.status.idle": "2024-09-30T17:35:01.435828Z",
     "shell.execute_reply": "2024-09-30T17:35:01.434615Z"
    },
    "papermill": {
     "duration": 0.01906,
     "end_time": "2024-09-30T17:35:01.438659",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.419599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\\nfrom langchain_chroma import Chroma\\n\\nfrom langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# otros\\nfrom collections import OrderedDict\\nfrom prettytable import PrettyTable\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# otros\n",
    "from collections import OrderedDict\n",
    "from prettytable import PrettyTable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d7701",
   "metadata": {
    "papermill": {
     "duration": 0.007209,
     "end_time": "2024-09-30T17:35:01.452997",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.445788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Embedding Model and Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55b0c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.469625Z",
     "iopub.status.busy": "2024-09-30T17:35:01.468745Z",
     "iopub.status.idle": "2024-09-30T17:35:01.476649Z",
     "shell.execute_reply": "2024-09-30T17:35:01.475462Z"
    },
    "papermill": {
     "duration": 0.019546,
     "end_time": "2024-09-30T17:35:01.479637",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.460091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# We create a Chroma instance, give it a embedding model\\nembedding_model = HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-MiniLM-L6-v2\")\\nvector_store = Chroma(embedding_function= embedding_model, persist_directory= data_directory)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# We create a Chroma instance, give it a embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma(embedding_function= embedding_model, persist_directory= data_directory)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33119a",
   "metadata": {
    "papermill": {
     "duration": 0.007308,
     "end_time": "2024-09-30T17:35:01.494674",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.487366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Hugging Face Hub Intro** üë®‚Äçüíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1cea83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.511939Z",
     "iopub.status.busy": "2024-09-30T17:35:01.511488Z",
     "iopub.status.idle": "2024-09-30T17:35:01.518461Z",
     "shell.execute_reply": "2024-09-30T17:35:01.517377Z"
    },
    "papermill": {
     "duration": 0.018917,
     "end_time": "2024-09-30T17:35:01.520881",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.501964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%capture\\n# Hugging Face Hub libraries\\n!pip install langchain langchain-core langchain-huggingface\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%capture\n",
    "# Hugging Face Hub libraries\n",
    "!pip install langchain langchain-core langchain-huggingface\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b92228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.538086Z",
     "iopub.status.busy": "2024-09-30T17:35:01.537088Z",
     "iopub.status.idle": "2024-09-30T17:35:01.542448Z",
     "shell.execute_reply": "2024-09-30T17:35:01.541456Z"
    },
    "papermill": {
     "duration": 0.01688,
     "end_time": "2024-09-30T17:35:01.545151",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.528271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# versions of libraries\n",
    "#!pip uninstall langchain-huggingface -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa09465e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.562806Z",
     "iopub.status.busy": "2024-09-30T17:35:01.562359Z",
     "iopub.status.idle": "2024-09-30T17:35:01.570238Z",
     "shell.execute_reply": "2024-09-30T17:35:01.568818Z"
    },
    "papermill": {
     "duration": 0.019157,
     "end_time": "2024-09-30T17:35:01.572750",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.553593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb51e5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.589952Z",
     "iopub.status.busy": "2024-09-30T17:35:01.589543Z",
     "iopub.status.idle": "2024-09-30T17:35:01.597414Z",
     "shell.execute_reply": "2024-09-30T17:35:01.595906Z"
    },
    "papermill": {
     "duration": 0.019504,
     "end_time": "2024-09-30T17:35:01.599919",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.580415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\\n\\naccess_token = \"chatrag_token\" # don\\'t forget to add hugging face token\\n\\nif access_token is None:\\n pass\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
    "\n",
    "access_token = \"chatrag_token\" # don't forget to add hugging face token\n",
    "\n",
    "if access_token is None:\n",
    " pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b40503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.617882Z",
     "iopub.status.busy": "2024-09-30T17:35:01.617489Z",
     "iopub.status.idle": "2024-09-30T17:35:01.624672Z",
     "shell.execute_reply": "2024-09-30T17:35:01.623433Z"
    },
    "papermill": {
     "duration": 0.019376,
     "end_time": "2024-09-30T17:35:01.627187",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.607811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdf9471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.644875Z",
     "iopub.status.busy": "2024-09-30T17:35:01.644412Z",
     "iopub.status.idle": "2024-09-30T17:35:01.651708Z",
     "shell.execute_reply": "2024-09-30T17:35:01.650538Z"
    },
    "papermill": {
     "duration": 0.019354,
     "end_time": "2024-09-30T17:35:01.654593",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.635239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrepo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\\n\\nllm = HuggingFaceEndpoint(\\n        repo_id= repo_id,\\n        max_new_tokens= 128,\\n        temperature= 0.5,\\n        huggingfacehub_api_token= access_token,\\n)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "        repo_id= repo_id,\n",
    "        max_new_tokens= 128,\n",
    "        temperature= 0.5,\n",
    "        huggingfacehub_api_token= access_token,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8cc3153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.672970Z",
     "iopub.status.busy": "2024-09-30T17:35:01.672531Z",
     "iopub.status.idle": "2024-09-30T17:35:01.681042Z",
     "shell.execute_reply": "2024-09-30T17:35:01.679725Z"
    },
    "papermill": {
     "duration": 0.021022,
     "end_time": "2024-09-30T17:35:01.683913",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.662891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt = PromptTemplate.from_template(template)\\n\\nllm_chain = prompt | llm\\nprint(llm_chain.invoke({\"question\": question}))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = prompt | llm\n",
    "print(llm_chain.invoke({\"question\": question}))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ccd1a8",
   "metadata": {
    "papermill": {
     "duration": 0.008291,
     "end_time": "2024-09-30T17:35:01.700490",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.692199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Retrieval System**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc99cc",
   "metadata": {
    "papermill": {
     "duration": 0.008096,
     "end_time": "2024-09-30T17:35:01.717539",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.709443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[Exploring Hugging Face Hub](https://medium.com/@punya8147_26846/exploring-hugging-face-hub-an-introduction-to-language-models-and-langchain-6a3998b3fd2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562b76f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.735602Z",
     "iopub.status.busy": "2024-09-30T17:35:01.735089Z",
     "iopub.status.idle": "2024-09-30T17:35:01.743226Z",
     "shell.execute_reply": "2024-09-30T17:35:01.742031Z"
    },
    "papermill": {
     "duration": 0.019932,
     "end_time": "2024-09-30T17:35:01.745700",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.725768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrepo_id_1 = \"mistralai/Mistral-7B-Instruct-v0.2\"\\nrepo_id_2=\"mistralai/Mistral-7B-Instruct-v0.3\"\\n\\nllm = HuggingFaceEndpoint(\\n        repo_id= repo_id_1,\\n        max_new_tokens= 1024,\\n        temperature= 1,\\n        huggingfacehub_api_token= access_token,\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "repo_id_1 = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "repo_id_2=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "        repo_id= repo_id_1,\n",
    "        max_new_tokens= 1024,\n",
    "        temperature= 1,\n",
    "        huggingfacehub_api_token= access_token,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "210e6cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.764466Z",
     "iopub.status.busy": "2024-09-30T17:35:01.763888Z",
     "iopub.status.idle": "2024-09-30T17:35:01.773620Z",
     "shell.execute_reply": "2024-09-30T17:35:01.772567Z"
    },
    "papermill": {
     "duration": 0.022481,
     "end_time": "2024-09-30T17:35:01.776631",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.754150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt_template =\\n\\nAs a highly knowledgeable fashion assistant, your role is to accurately interpret fashion queries and \\nprovide responses using our specialized fashion database. Follow these directives to ensure optimal user interactions:\\n1. Precision in Answers: Respond solely with information directly relevant to the user\\'s query from our fashion database. \\n    Refrain from making assumptions or adding extraneous details.\\n2. Topic Relevance: Limit your expertise to specific fashion-related areas:\\n    - Fashion Trends\\n    - Personal Styling Advice\\n    - Seasonal Wardrobe Selections\\n    - Accessory Recommendations\\n3. Handling Off-topic Queries: For questions unrelated to fashion (e.g., general knowledge questions like \"Why is the sky blue?\"), \\n    politely inform the user that the query is outside the chatbot‚Äôs scope and suggest redirecting to fashion-related inquiries.\\n4. Promoting Fashion Awareness: Craft responses that emphasize good fashion sense, aligning with the latest trends and \\n    personalized style recommendations.\\n5. Contextual Accuracy: Ensure responses are directly related to the fashion query, utilizing only pertinent \\n    information from our database.\\n6. Relevance Check: If a query does not align with our fashion database, guide the user to refine their \\n    question or politely decline to provide an answer.\\n7. Avoiding Duplication: Ensure no response is repeated within the same interaction, maintaining uniqueness and \\n    relevance to each user query.\\n8. Streamlined Communication: Eliminate any unnecessary comments or closing remarks from responses. Focus on\\n    delivering clear, concise, and direct answers.\\n9. Avoid Non-essential Sign-offs: Do not include any sign-offs like \"Best regards\" or \"FashionBot\" in responses.\\n10. One-time Use Phrases: Avoid using the same phrases multiple times within the same response. Each \\n    sentence should be unique and contribute to the overall message without redundancy.\\n\\nFashion Query:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "prompt_template =\n",
    "\n",
    "As a highly knowledgeable fashion assistant, your role is to accurately interpret fashion queries and \n",
    "provide responses using our specialized fashion database. Follow these directives to ensure optimal user interactions:\n",
    "1. Precision in Answers: Respond solely with information directly relevant to the user's query from our fashion database. \n",
    "    Refrain from making assumptions or adding extraneous details.\n",
    "2. Topic Relevance: Limit your expertise to specific fashion-related areas:\n",
    "    - Fashion Trends\n",
    "    - Personal Styling Advice\n",
    "    - Seasonal Wardrobe Selections\n",
    "    - Accessory Recommendations\n",
    "3. Handling Off-topic Queries: For questions unrelated to fashion (e.g., general knowledge questions like \"Why is the sky blue?\"), \n",
    "    politely inform the user that the query is outside the chatbot‚Äôs scope and suggest redirecting to fashion-related inquiries.\n",
    "4. Promoting Fashion Awareness: Craft responses that emphasize good fashion sense, aligning with the latest trends and \n",
    "    personalized style recommendations.\n",
    "5. Contextual Accuracy: Ensure responses are directly related to the fashion query, utilizing only pertinent \n",
    "    information from our database.\n",
    "6. Relevance Check: If a query does not align with our fashion database, guide the user to refine their \n",
    "    question or politely decline to provide an answer.\n",
    "7. Avoiding Duplication: Ensure no response is repeated within the same interaction, maintaining uniqueness and \n",
    "    relevance to each user query.\n",
    "8. Streamlined Communication: Eliminate any unnecessary comments or closing remarks from responses. Focus on\n",
    "    delivering clear, concise, and direct answers.\n",
    "9. Avoid Non-essential Sign-offs: Do not include any sign-offs like \"Best regards\" or \"FashionBot\" in responses.\n",
    "10. One-time Use Phrases: Avoid using the same phrases multiple times within the same response. Each \n",
    "    sentence should be unique and contribute to the overall message without redundancy.\n",
    "\n",
    "Fashion Query:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73a1ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.796345Z",
     "iopub.status.busy": "2024-09-30T17:35:01.795919Z",
     "iopub.status.idle": "2024-09-30T17:35:01.800768Z",
     "shell.execute_reply": "2024-09-30T17:35:01.799662Z"
    },
    "papermill": {
     "duration": 0.017781,
     "end_time": "2024-09-30T17:35:01.803443",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.785662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom promtp template\n",
    "#custom_prompt = PromptTemplate(template= prompt_template, input_variables= [\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4276817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T17:35:01.823782Z",
     "iopub.status.busy": "2024-09-30T17:35:01.823317Z",
     "iopub.status.idle": "2024-09-30T17:35:01.830583Z",
     "shell.execute_reply": "2024-09-30T17:35:01.829466Z"
    },
    "papermill": {
     "duration": 0.021085,
     "end_time": "2024-09-30T17:35:01.833715",
     "exception": false,
     "start_time": "2024-09-30T17:35:01.812630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrag_chain = RetrievalQA.from_chain_type(\\n    llm= llm, \\n    chain_type= \"stuff\", \\n    retriever= vector_store.as_retriever(top_k=3),\\n    chain_type_kwargs= {\"prompt\": custom_prompt})\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm= llm, \n",
    "    chain_type= \"stuff\", \n",
    "    retriever= vector_store.as_retriever(top_k=3),\n",
    "    chain_type_kwargs= {\"prompt\": custom_prompt})\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.971429,
   "end_time": "2024-09-30T17:35:02.264227",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-30T17:34:58.292798",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
